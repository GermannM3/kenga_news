import aiohttp
import re
from cachetools import cached, TTLCache
import logging
from dotenv import load_dotenv
import os
from aiogram import Bot
import asyncio
from bot.database import is_news_published, add_news_to_db

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

logger = logging.getLogger(__name__)

# API-–∫–ª—é—á –∏ –¥—Ä—É–≥–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
NEWS_API_KEY = os.getenv("NEWS_API_KEY")
KEYWORDS = ["xbox", "AI", "–ö–ù–î–†", "–†–æ—Å—Å–∏—è", "—ç–∫–æ–Ω–æ–º–∏–∫–∞", "–∫–æ—Å–º–æ—Å"]
HASHTAGS = {kw.lower(): f"#{kw.lower()}" for kw in KEYWORDS}

# –ö—ç—à –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API (10 –º–∏–Ω—É—Ç)
cache = TTLCache(maxsize=100, ttl=600)
publish_cache = TTLCache(maxsize=1, ttl=30)  # –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: 30 —Å–µ–∫—É–Ω–¥

def clean_text(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –Ω–∞—Ä—É—à–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Markdown –∏–ª–∏ HTML."""
    return re.sub(r"[\*_\[\]()]", "", text) if text else ""

@cached(cache)
async def fetch_news(keyword):
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É."""
    url = f"https://newsapi.org/v2/everything?q={keyword}&apiKey={NEWS_API_KEY}&language=ru&sortBy=publishedAt&pageSize=5"
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                return await response.json()
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {response.status}")
            return {"articles": []}

async def publish_news(bot: Bot):
    """–ü—É–±–ª–∏–∫—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ –∫–∞–Ω–∞–ª Telegram —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏."""
    try:
        for keyword in KEYWORDS:
            news_data = await fetch_news(keyword)
            articles = news_data.get("articles", [])

            for article in articles:
                title = clean_text(article.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"))
                description = clean_text(article.get("description", "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è"))
                url = article.get("url", "#")
                image_url = article.get("urlToImage", "")

                if is_news_published(title):
                    logger.info(f"–ù–æ–≤–æ—Å—Ç—å —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞: {title}")
                    continue

                relevant_hashtags = [
                    HASHTAGS[key] for key in HASHTAGS
                    if key in title.lower() or key in description.lower()
                ]
                hashtags = " ".join(relevant_hashtags) if relevant_hashtags else ""

                message = (
                    f"<b>{title}</b>\n\n{description}\n\n<a href='{url}'>–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ</a>\n\n"
                    f"{hashtags}\n\nü¶ò –ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è: @kenga_news"
                )

                if "last_published" in publish_cache:
                    await asyncio.sleep(30)

                try:
                    if image_url:
                        await bot.send_photo(
                            chat_id=os.getenv("PUBLICATION_CHANNEL_ID"),
                            photo=image_url,
                            caption=message,
                            parse_mode="HTML"
                        )
                    else:
                        await bot.send_message(
                            chat_id=os.getenv("PUBLICATION_CHANNEL_ID"),
                            text=message,
                            parse_mode="HTML"
                        )
                    add_news_to_db(title)
                    publish_cache["last_published"] = True
                    logger.info(f"–ù–æ–≤–æ—Å—Ç—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞: {title}")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")

                await asyncio.sleep(30)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
